Metadata-Version: 2.4
Name: prompt-efficiency-suite
Version: 0.1.0
Summary: A comprehensive toolkit for analyzing and optimizing prompts for large language models
Home-page: https://github.com/prompt-efficiency/prompt-efficiency-suite
Author: Prompt Efficiency Team
Author-email: team@prompt-efficiency.com
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: click>=8.0.0
Requires-Dist: numpy>=1.20.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: scikit-learn>=0.24.0
Requires-Dist: torch>=1.9.0
Requires-Dist: transformers>=4.0.0
Requires-Dist: tqdm>=4.60.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Prompt Efficiency Suite

A comprehensive toolkit for analyzing and optimizing prompts for large language models (LLMs).

## Overview

The Prompt Efficiency Suite provides tools and utilities for analyzing prompts across various dimensions including clarity, structure, complexity, and effectiveness. It helps users understand and improve their prompts through detailed metrics and suggestions.

## Features

### Core Components

1. **Prompt Analyzer**
   - Analyzes prompts for clarity, structure, and complexity
   - Provides detailed metrics and improvement suggestions
   - Supports multiple analysis dimensions

2. **Model Translator**
   - Handles prompt translation between different LLM formats
   - Supports major models including GPT-4, GPT-3.5, Claude, PaLM, LLaMA, and Mistral
   - Maintains prompt integrity across translations

3. **Pattern Recognition**
   - Identifies common prompt patterns and structures
   - Supports various prompt engineering techniques
   - Provides pattern-specific optimization suggestions

### Analysis Dimensions

1. **Clarity Metrics**
   - Clarity score
   - Specificity score
   - Completeness score

2. **Structure Metrics**
   - Structure score
   - Organization score
   - Coherence score

3. **Complexity Metrics**
   - Complexity score
   - Technical depth score
   - Scope score

4. **Effectiveness Metrics**
   - Effectiveness score
   - Impact score
   - Measurability score

## Installation

```bash
pip install prompt-efficiency-suite
```

## Usage

### Basic Usage

```python
from prompt_efficiency_suite import PromptAnalyzer

analyzer = PromptAnalyzer()
result = analyzer.analyze_prompt(
    prompt="Your prompt here",
    metrics=["clarity", "structure", "complexity"]
)
```

### Advanced Usage

```python
from prompt_efficiency_suite import PromptAnalyzer
from prompt_efficiency_suite.model_translator import ModelType

analyzer = PromptAnalyzer()
result = analyzer.analyze_prompt(
    prompt="Your prompt here",
    model=ModelType.GPT4,
    metrics=["clarity", "structure", "complexity"]
)
```

## Example Scripts

The suite includes several example scripts demonstrating different aspects of prompt analysis:

1. **analyze_different_models.py**
   - Demonstrates prompt analysis across different LLM models
   - Shows how analysis results vary by model type

2. **analyze_different_metrics.py**
   - Shows how to analyze prompts using different metric combinations
   - Demonstrates the impact of metric selection on analysis results

3. **analyze_different_patterns.py**
   - Illustrates analysis of various prompt patterns
   - Includes examples of chain-of-thought, few-shot, and zero-shot prompts

4. **analyze_different_constraints.py**
   - Demonstrates how to analyze prompts with different types of constraints
   - Shows the impact of constraints on prompt effectiveness

5. **analyze_different_outputs.py**
   - Shows how to analyze prompts with different expected output formats
   - Demonstrates analysis for text, code, JSON, and other output types

6. **analyze_prompt_metrics.py**
   - Comprehensive example of using different metrics and evaluation criteria
   - Shows how to interpret and use analysis results

## Analysis Results

The analyzer provides detailed results including:

- Numerical scores for each metric
- Improvement suggestions
- Pattern analysis
- Token estimates
- Quality scores

## Best Practices

1. **Prompt Structure**
   - Use clear sections (System, Context, Instruction, Example)
   - Maintain consistent formatting
   - Include relevant constraints and requirements

2. **Metric Selection**
   - Choose metrics relevant to your use case
   - Consider the target model's capabilities
   - Balance between different metric types

3. **Pattern Usage**
   - Select appropriate patterns for your task
   - Combine patterns effectively
   - Consider model-specific pattern effectiveness

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For support, please open an issue in the GitHub repository or contact the maintainers. 
